{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re,glob,os,secrets\n",
    "import urllib.parse\n",
    "import urllib.request\n",
    "from multiprocessing.pool import ThreadPool\n",
    "import urllib\n",
    "import pandas as pd\n",
    "import operator\n",
    "import functools\n",
    "from operator import methodcaller\n",
    "\n",
    "URL = 'https://www.uniprot.org/uploadlists/'\n",
    "\n",
    "def create_chunks(list_name, n):\n",
    "    for i in range(0, len(list_name), n):\n",
    "        yield list_name[i:i + n]\n",
    "        \n",
    "def mapping_ids(ids2map, source_fmt='P_GI',target_fmt='ACC', output_fmt='tab'):\n",
    "    if hasattr(ids2map, 'pop'):\n",
    "            ids2map = ' '.join(ids2map)\n",
    "    params = {\n",
    "    'from': source_fmt,\n",
    "    'to': target_fmt,\n",
    "    'format': output_fmt,\n",
    "    'query': ids2map\n",
    "    }\n",
    "\n",
    "    data = urllib.parse.urlencode(params)\n",
    "    data = data.encode('utf-8')\n",
    "    req = urllib.request.Request(URL, data)\n",
    "    with urllib.request.urlopen(req) as f:\n",
    "        response = f.read().decode('utf-8')\n",
    "    return response\n",
    "\n",
    "def concat_dfs(path,extension):\n",
    "    extension = extension\n",
    "    all_filenames = [i for i in glob.glob('*.{}'.format(extension))]\n",
    "    \n",
    "    combined_dfs = pd.concat([pd.read_table(f) for f in all_filenames ])\n",
    "    \n",
    "    return combined_dfs\n",
    "\n",
    "def taxonomy_metag(basta_output, besthits):\n",
    "    columns_basta_output = ['id_read','taxonomy']\n",
    "    basta_output = pd.DataFrame(np.vstack([basta_output.columns,basta_output]))\n",
    "    basta_output.columns = columns_basta_output\n",
    "    taxonomy = basta_output[\"taxonomy\"].str.split(\";\",expand=True)\n",
    "    basta_output[\"taxonomy\"] = taxonomy[5]\n",
    "    besthits = pd.merge(besthits,basta_output,left_on=\"query_name\",right_on=\"id_read\",how=\"left\")\n",
    "    besthits = besthits.drop(\"id_read\",1)\n",
    "    besthits = besthits.loc[besthits[\"taxonomy\"]!=\"unknown\"]\n",
    "    besthits = besthits.loc[besthits[\"taxonomy\"].notna()]\n",
    "    besthits = besthits.reset_index(drop=True)\n",
    "    besthits = besthits[~besthits.duplicated(['KO','taxonomy'])].reset_index(drop=True)\n",
    "    return besthits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_besthits = pd.read_table(\"C:/Users/eulle/Documents/metagenomica/marbella/besthits_E7.m8\")\n",
    "\n",
    "columns_csv = ['qseqid' ,'sseqid' ,'pident' ,'length','mismatch','gapopen','qstart' ,'qend' ,'sstart' ,'send' ,'evalue' ,'bitscore']\n",
    "row_values = pd.Series(df_besthits.columns)\n",
    "df_besthits = pd.DataFrame(np.vstack([df_besthits.columns,df_besthits]))\n",
    "df_besthits.columns = columns_csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "gi_number = [re.findall(r\"(?<=GI:|gi:).*?(?=\\||p)\", ii[1]['sseqid']) for ii in df_besthits.iterrows()]\n",
    "gi_number = functools.reduce(operator.iconcat, gi_number, [])\n",
    "gi_number = list(set(gi_number))\n",
    "\n",
    "uniprot_id = [re.findall(r\"(?<=BL:|ot:).*?(?=\\||p)\",ii[1]['sseqid']) for ii in df_besthits.iterrows()]\n",
    "uniprot_id = functools.reduce(operator.iconcat, uniprot_id, [])\n",
    "uniprot_id = list(set(uniprot_id))\n",
    "\n",
    "chunk_gi = list(create_chunks(gi_number,1000))\n",
    "chunk_uniprot = list(create_chunks(uniprot_id,1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma função para escrever os arquivos\n",
    "map_gi = ThreadPool(10).imap(mapping_ids,chunk_gi)\n",
    "map_uniprot = ThreadPool(10).imap(functools.partial(mapping_ids,source_fmt='ACC',target_fmt='KO_ID'),chunk_uniprot)\n",
    "\n",
    "parent_dir = \"D:/eulle/importante/\"\n",
    "dir_gi_uni = \"gi_number-uni\"\n",
    "dir_uni_ko = \"uniprot-ko_id\"\n",
    "\n",
    "if(os.path.isdir('D:/eulle/importante/gi_number-uni/')==False):\n",
    "    os.mkdir(os.path.join(parent_dir, dir_gi_uni))\n",
    "    for index,ii in enumerate(map_gi):\n",
    "        with open('D:/eulle/importante/gi_number-uni/' + secrets.token_hex(15) +'.csv', 'w') as file:\n",
    "            file.write(ii)\n",
    "else:\n",
    "    for index,ii in enumerate(map_gi):\n",
    "        with open('D:/eulle/importante/gi_number-uni/' + secrets.token_hex(15) +'.csv', 'w') as file:\n",
    "            file.write(ii)\n",
    "            \n",
    "            \n",
    "if(os.path.isdir('D:/eulle/importante/uniprot-ko_id/')==False):        \n",
    "    os.mkdir(os.path.join(parent_dir, dir_uni_ko))\n",
    "    for index,ii in enumerate(map_uniprot): \n",
    "        with open('D:/eulle/importante/uniprot-ko_id/' + secrets.token_hex(15) +'.csv', 'w') as file:\n",
    "            file.write(ii)\n",
    "else:\n",
    "    for index,ii in enumerate(map_gi):\n",
    "        with open('D:/eulle/importante/uniprot-ko_id/' + secrets.token_hex(15) +'.csv', 'w') as file:\n",
    "            file.write(ii)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pensar em uma forma de melhorar esses downloads\n",
    "gi_uniprot_concat = concat_dfs(os.chdir(\"D:/eulle/importante/gi_number-uni/\"),\"csv\")\n",
    "gi_uniprot_concat = gi_uniprot_concat.drop_duplicates(\"From\")\n",
    "gi_uniprot_concat = gi_uniprot_concat.reset_index(drop=True)\n",
    "gi_uniprot_concat.columns = [\"GI\",\"Uniprot_ID\"]\n",
    "\n",
    "gi_uniprot_concat.to_csv(\"D:/eulle/importante/df_gi_uniprot-E7.csv\",sep=\"\\t\",encoding=\"utf-8\",index=False)\n",
    "\n",
    "list_gi_uniprot = list(gi_uniprot_concat[\"Uniprot_ID\"])\n",
    "chunk_uniprot = list(create_chunks(list_gi_uniprot,1000))\n",
    "map_uniprot = ThreadPool(10).imap(functools.partial(mapping_ids,source_fmt='ACC',target_fmt='KO_ID'),chunk_uniprot)\n",
    "\n",
    "for index,ii in enumerate(map_uniprot): \n",
    "    with open('D:/eulle/importante/uniprot-ko_id/'+ secrets.token_hex(15) +'.csv', 'w') as file:\n",
    "        file.write(ii)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot_ko_concat = concat_dfs(os.chdir(\"D:/eulle/importante/uniprot-ko_id/\"),\"csv\")\n",
    "uniprot_ko_concat.columns = [\"Uniprot_ID\",\"KO\"]\n",
    "uniprot_ko_concat = uniprot_ko_concat.reset_index(drop=True)\n",
    "\n",
    "uniprot_ko_concat.to_csv(\"D:/eulle/importante/df_ko_ids_E7.csv\",sep=\"\\t\",encoding=\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "basta_output = pd.read_fwf(\"C:/Users/eulle/Documents/metagenomica/besthit_CoSQG.txt\")\n",
    "besthits = pd.read_csv(\"C:/Users/eulle/Documents/metagenomica/besthits_pathways-CoSQG-julia.csv\",sep=\"]\")\n",
    "taxmy = taxonomy_metag(basta_output,besthits)\n",
    "#taxmy.to_csv(\"C:/Users/eulle/Documents/metagenomica/taxonomy_besthits-CoSQG.csv\",sep=\"]\",encoding=\"utf-8\",index=False)\n",
    "\n",
    "\n",
    "# É SÓ PEGAR O UNIQUE DA TABELA TAXONOMY E EXCLUIR OS UNKNOWN -- FEITO\n",
    "# SABER QUAIS KOs NÀO FORAM MAPEADOS E CONTAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'taxmy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-039cdb598366>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtaxmy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"C:/Users/eulle/Documents/metagenomica/taxonomy_besthits-CoSQG.csv\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"]\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'taxmy' is not defined"
     ]
    }
   ],
   "source": [
    "taxmy.to_csv(\"C:/Users/eulle/Documents/metagenomica/taxonomy_besthits-CoSQG.csv\",sep=\"]\",encoding=\"utf-8\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxmy = pd.read_csv(\"D:/Downloads/df_eggnog_final.csv\",sep=\",\")\n",
    "taxmy = taxmy[~taxmy.duplicated(['KEGG_ko','best_tax_level'])].reset_index(drop=True)\n",
    "#taxmy.drop_duplicates('KEGG_ko',inplace=True)\n",
    "\n",
    "#taxmy.to_csv(\"D:/Downloads/taxmy_CoSqG.csv\",encoding=\"utf-8\",sep=\",\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxmy = taxmy[pd.isna(taxmy[\"KEGG_Pathway\"])==False].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "taxmy.to_csv(\"D:/Downloads/taxmy_CoSqG\",encoding=\"utf-8\",sep=\"\\t\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
